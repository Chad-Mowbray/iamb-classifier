{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f50e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main started...\n",
      "create accented word dict started...\n",
      "get accented words started...\n",
      "on period:  elizabethan\n",
      "get sections per period started...\n",
      "create accebted word feature started...\n",
      "reset started...\n",
      "on period:  neoclassical\n",
      "get sections per period started...\n",
      "create accebted word feature started...\n",
      "reset started...\n",
      "on period:  romantic\n",
      "get sections per period started...\n",
      "create accebted word feature started...\n",
      "reset started...\n",
      "on period:  victorian\n",
      "get sections per period started...\n",
      "create accebted word feature started...\n",
      "reset started...\n",
      "combine all period features started...\n",
      "get train test split started...\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from copy import deepcopy\n",
    "from random import shuffle\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from dataprep import RawFileProcessor\n",
    "from runner import Runner\n",
    "\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "    PERIODS = (\n",
    "        \"elizabethan\",\n",
    "        \"neoclassical\",\n",
    "        \"romantic\",\n",
    "        \"victorian\"\n",
    "    )\n",
    "    SECTION_LENGTH = 100\n",
    "\n",
    "    def __init__(self, accented_words_file):\n",
    "        # self.period_pickle_files = period_pickle_files\n",
    "        self.accented_words_file = accented_words_file\n",
    "        self.ordered_accented_words = {}\n",
    "        self.counter_dicts = []\n",
    "        self.other_features = []\n",
    "        self.all_period_features = []\n",
    "\n",
    "        self.main()\n",
    "\n",
    "\n",
    "    def get_accented_words(self):\n",
    "        print('get accented words started...')\n",
    "\n",
    "        with open(self.accented_words_file, 'r') as f:\n",
    "            accented_words = [word.rstrip(\"\\n\") for word in f.readlines()]\n",
    "            return accented_words\n",
    "\n",
    "\n",
    "    def create_accented_word_dict(self):\n",
    "        print('create accented word dict started...')\n",
    "\n",
    "        accented_words = self.get_accented_words()\n",
    "        unordered_accented_words = Counter(accented_words)\n",
    "        ordered_accented_words = OrderedDict(unordered_accented_words)\n",
    "        self.ordered_accented_words = ordered_accented_words\n",
    "\n",
    "\n",
    "    def create_fresh_word_dict(self):\n",
    "        print('create fresh word dict started...')\n",
    "\n",
    "        ordered_accented_words_copy = deepcopy(self.ordered_accented_words)\n",
    "        fresh_word_dict = OrderedDict({k:0 for k in ordered_accented_words_copy})\n",
    "        return fresh_word_dict\n",
    "\n",
    "    \n",
    "    def get_sections_per_period(self, period):\n",
    "        print('get sections per period started...')\n",
    "\n",
    "        filename = os.path.join(globals()['_dh'][0], f\"poems/{period}_poems.txt\")\n",
    "        rfp = RawFileProcessor(filename)\n",
    "        contents = rfp.cleaned_contents\n",
    "\n",
    "        shuffle(contents)\n",
    "        sectioned_contents = []\n",
    "        sections = []\n",
    "        for i,line in enumerate(contents):\n",
    "            if i == 0: continue\n",
    "            sections.append(line.rstrip(\"\\n\"))\n",
    "            if i % self.SECTION_LENGTH == 0:\n",
    "                sectioned_contents.append(sections)\n",
    "                sections = []\n",
    "\n",
    "        for i,section in enumerate(sectioned_contents):\n",
    "            r = Runner(section)\n",
    "            features = r.initial_process_contents(period)\n",
    "            counter_dict = features[\"counter_dict\"]\n",
    "            rules_avg = features[\"rules_avg\"]\n",
    "            words_per_line = features[\"words_per_line\"]\n",
    "            avg_syllables_per_line = features[\"avg_syllables_per_line\"]\n",
    "            rule_0 = features[\"rule_0\"]\n",
    "            rule_1 = features[\"rule_1\"]\n",
    "            rule_2 = features[\"rule_2\"]\n",
    "            rule_3 = features[\"rule_3\"]\n",
    "            rule_4 = features[\"rule_4\"]\n",
    "            rule_5 = features[\"rule_5\"]\n",
    "            rule_6 = features[\"rule_6\"]\n",
    "            self.counter_dicts.append(counter_dict)\n",
    "            self.other_features = [rules_avg, words_per_line, avg_syllables_per_line, rule_0, rule_1, rule_2, rule_3, rule_4, rule_5, rule_6] \n",
    "\n",
    "\n",
    "    def create_accented_word_feature(self, period):\n",
    "        print('create accebted word feature started...')\n",
    "\n",
    "        period_features = []\n",
    "        for i,sect in enumerate(self.counter_dicts):\n",
    "            sect = sorted([word for word in sect])\n",
    "            sect_dict = OrderedDict(Counter(sect))\n",
    "            sect_combined = OrderedDict()\n",
    "            for k,v in self.ordered_accented_words.items():\n",
    "                if k in sect_dict:\n",
    "                    sect_combined[k] = 1\n",
    "                else:\n",
    "                    sect_combined[k] = 0\n",
    "            one_hot_sect = [[float(v) for v in sect_combined.values()]]\n",
    "            one_hot_sect[0].extend(self.other_features)\n",
    "            one_hot_sect.append(period)\n",
    "            period_features.append(one_hot_sect)\n",
    "\n",
    "        return period_features\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        print('reset started...')\n",
    "        self.counter_dicts = []\n",
    "        self.other_features = []\n",
    "\n",
    "\n",
    "    def combine_all_period_features(self):\n",
    "        print('combine all period features started...')\n",
    "        flattened_all_period_features = [sect for period in self.all_period_features for sect in period]\n",
    "        shuffle(flattened_all_period_features)\n",
    "        return flattened_all_period_features\n",
    "\n",
    "\n",
    "    def get_train_test_split(self, flattened_all_period_features):\n",
    "        print('get train test split started...')\n",
    "        X = [x[0] for x in flattened_all_period_features]\n",
    "        y = [x[1] for x in flattened_all_period_features]\n",
    "\n",
    "        size = len(X)\n",
    "        if size != len(y): raise Exception(\"X and y not same len\")\n",
    "        test_split_point = size // 4\n",
    "        X_train = X[test_split_point:]\n",
    "        y_train = y[test_split_point:]\n",
    "        X_train_np = np.array(X_train)\n",
    "        y_train_np = np.array(y_train)\n",
    "\n",
    "        X_test = X[:test_split_point]\n",
    "        y_test = y[:test_split_point]\n",
    "        X_test_np = np.array(X_test)\n",
    "        y_test_np = np.array(y_test)\n",
    "\n",
    "        return {\n",
    "            \"X_test_np\": X_test_np,\n",
    "            \"y_test_np\": y_test_np,\n",
    "            \"X_train_np\": X_train_np,\n",
    "            \"y_train_np\": y_train_np\n",
    "        }\n",
    "\n",
    "\n",
    "    def train_model(self, train_test):\n",
    "        print('train model started...')\n",
    "        models = [MultinomialNB, ComplementNB, MLPClassifier]\n",
    "        names = [\"MultinomialNB\", \"ComplementNB\", \"MLPClassifier\"]\n",
    "        for i,model in enumerate(models):\n",
    "            print(str(model))\n",
    "            print( len(train_test[\"X_train_np\"]), len(train_test[\"y_train_np\"]) )\n",
    "\n",
    "\n",
    "            params = dict()\n",
    "            params['alpha'] = (1e-6, 100.0, 'log-uniform')\n",
    "\n",
    "            # define evaluation\n",
    "            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "            # define the search\n",
    "            search = BayesSearchCV(estimator=model(), search_spaces=params, n_jobs=-1, cv=cv)\n",
    "            # perform the search\n",
    "            search.fit(train_test[\"X_train_np\"], train_test[\"y_train_np\"])\n",
    "            # report the best result\n",
    "            print(search.best_score_)\n",
    "            print(search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # test_model = model()\n",
    "            # test_model.fit(train_test[\"X_train_np\"], train_test[\"y_train_np\"])\n",
    "            # with open(f'{names[i]}_test-trained-model.pickle', 'wb') as f:\n",
    "            #     pickle.dump(test_model, f)\n",
    "\n",
    "\n",
    "    def test_model(self, train_test):\n",
    "        print('test model started...')\n",
    "        models = [\"MultinomialNB\", \"ComplementNB\", \"MLPClassifier\"]\n",
    "        for model in models:\n",
    "            with open(f\"{model}_test-trained-model.pickle\", 'rb') as f:\n",
    "                print(f\"{model}...\", \"\\n\")\n",
    "                test_model = pickle.load(f)\n",
    "                predicted = test_model.predict(train_test[\"X_test_np\"])\n",
    "                print(metrics.classification_report(train_test[\"y_test_np\"], predicted))\n",
    "                print(metrics.confusion_matrix(train_test[\"y_test_np\"], predicted))\n",
    "                print(metrics.accuracy_score(train_test[\"y_test_np\"], predicted))\n",
    "\n",
    "                print(\"Accuracy on training set: {:.3f}\".format(test_model.score(train_test[\"X_train_np\"], train_test[\"y_train_np\"])))\n",
    "                print(\"Accuracy on test set: {:.3f}\".format(test_model.score(train_test[\"X_test_np\"], train_test[\"y_test_np\"])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        print('main started...')\n",
    "        self.create_accented_word_dict()\n",
    "        for period in self.PERIODS:\n",
    "            print(\"on period: \", period)\n",
    "            self.get_sections_per_period(period)\n",
    "            period_features = self.create_accented_word_feature(period)\n",
    "            self.all_period_features.append(period_features)\n",
    "            self.reset()\n",
    "        flattened_all_period_features = self.combine_all_period_features()\n",
    "        self.train_test = self.get_train_test_split(flattened_all_period_features)\n",
    "#         self.train_model(train_test)\n",
    "        # self.test_model(train_test)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "m = ModelTrainer(\"accented_words.txt\")\n",
    "train_test = m.train_test\n",
    "print(len(train_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44fe26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MultinomialNB(),\n",
       "             param_grid=[{'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 2, 3, 10, 100,\n",
       "                                    1000]}])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [\n",
    "  {'alpha': [.0001, .001, .01, .1, 1,2,3, 10, 100, 1000]},\n",
    " ]\n",
    "\n",
    "model = MultinomialNB()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(train_test[\"X_train_np\"], train_test[\"y_train_np\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f491a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_alpha',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d03cac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08b0da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "  {'alpha': [1,1.5,2, 2.5,3,4]},\n",
    " ]\n",
    "\n",
    "model = ComplementNB()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(train_test[\"X_train_np\"], train_test[\"y_train_np\"])\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3184d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codeplaton/Desktop/digital_humanities/Generals/iamb_classifier/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/codeplaton/Desktop/digital_humanities/Generals/iamb_classifier/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/codeplaton/Desktop/digital_humanities/Generals/iamb_classifier/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/codeplaton/Desktop/digital_humanities/Generals/iamb_classifier/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/codeplaton/Desktop/digital_humanities/Generals/iamb_classifier/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'identity',\n",
       " 'alpha': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'max_iter': 1000,\n",
       " 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "  {\n",
    "      'alpha': [.00000001,  .00001 , .001, 1],\n",
    "      \"hidden_layer_sizes\": [(100,), (100,10,10)],\n",
    "      \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "      \"solver\": [\"lbfgs\"],\n",
    "      \"max_iter\": [1000]\n",
    "  }\n",
    " ]\n",
    "\n",
    "model = MLPClassifier()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(train_test[\"X_train_np\"], train_test[\"y_train_np\"])\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params = {'activation': 'identity',\n",
    " 'alpha': 1e-08,\n",
    " 'hidden_layer_sizes': (100, 100, 100),\n",
    " 'solver': 'lbfgs'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
